{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32bcdb7d-96d5-47f5-a77a-139bafcc6a7d",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from myst_nb import glue\n",
    "from IPython.display import Markdown as md\n",
    "from slugify import slugify\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import plastockconf as psc\n",
    "from plastockconf import name_zones, name_particles, name_frequentation, name_situation\n",
    "from plastockconf import particle_groups, name_substrate, name_distance, table_css_styles, table_css_styles_top\n",
    "\n",
    "from plastock import attribute_summary, attribute_summary_test, attribute_summary_grid, add_table_to_page\n",
    "\n",
    "def translate_describe(x, value_column, transpose: bool = False):\n",
    "    described = x.to_dict()\n",
    "    described.pop(\"count\")\n",
    "    described[\"moyenne\"] = described.pop(\"mean\")\n",
    "    described[\"écart-type\"] = described.pop(\"std\")\n",
    "    df = pd.DataFrame(described.items())\n",
    "    df.set_index(0, inplace=True)\n",
    "    df.rename(columns={1:value_column}, inplace=True)\n",
    "    df.index.name = None\n",
    "    \n",
    "    if transpose:\n",
    "        df = df.T\n",
    "        \n",
    "    return df\n",
    "import reportclass as rc\n",
    "import plastock as pst\n",
    "language_maps = rc.language_maps()\n",
    "\n",
    "# a_property =  {'color' : 'red'}\n",
    "# format_kwargs = dict(precision=2, thousands=\"'\", decimal=\",\")\n",
    "glue('blank_caption', \" \", display=False)\n",
    "\n",
    "section = 'MP'\n",
    "page = \"\"\n",
    "\n",
    "work_data = pd.read_csv(\"data/end_pipe/long_form_micro.csv\")\n",
    "work_data.rename(columns={'echantillon':'échantillon', 'frequentation':'fréquentation'}, inplace=True)\n",
    "beach_data = pd.read_csv(\"data/end_pipe/asl_beaches.csv\")\n",
    "\n",
    "\n",
    "# the regional labels for each survey location\n",
    "regions = pd.read_csv(\"data/end_pipe/lac_leman_regions.csv\")\n",
    "regions.set_index('slug', drop=True, inplace=True)\n",
    "\n",
    "# the city name of the survey locations\n",
    "city_map = pd.read_csv('data/end_pipe/city_map.csv')\n",
    "city_map.set_index('slug', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006b1cf8-46b5-4359-ab2e-d5c0233efead",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ! combine souples et dur !\n",
    "fibers = work_data.loc[work_data.objet == 'fibres'].copy()\n",
    "not_fibers = work_data.loc[work_data.objet != 'fibres'].copy()\n",
    "\n",
    "not_fibers['objet'] = 'fragments'\n",
    "\n",
    "work_datai = pd.concat([fibers, not_fibers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e618b570-baa3-4688-8cc6-857866c14610",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "def name_the_new_distance(x, less='<= 500 m', more = '> 500 m'):\n",
    "    if x == 1:\n",
    "        return less\n",
    "    else:\n",
    "        return more\n",
    "\n",
    "def name_the_new_freq(x, new):\n",
    "    if x <= 2:\n",
    "        return new\n",
    "    else:\n",
    "        return 'Elévée'\n",
    "\n",
    "\n",
    "# the feature variables are added to the work_data\n",
    "ti = work_datai.copy()\n",
    "features = ['frequentation', 'situation', 'orientation', 'distance']\n",
    "\n",
    "work_datai['slug'] = work_datai.Plage.apply(lambda x: slugify(x))\n",
    "work_datai['region'] = work_datai.slug.apply(lambda x: regions.loc[x, 'alabel'])\n",
    "work_datai['city'] =  work_datai.slug.apply(lambda x: city_map.loc[x, 'city'])\n",
    "\n",
    "work_data['particules'] = work_data.compte\n",
    "work_datai[\"particules\"] = work_datai.compte\n",
    "\n",
    "work_data['slug'] = work_data.Plage.apply(lambda x: slugify(x))\n",
    "work_data['region'] = work_data.slug.apply(lambda x: regions.loc[x, 'alabel'])\n",
    "work_data['city'] =  work_data.slug.apply(lambda x: city_map.loc[x, 'city'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31df0293-bb2f-4aa2-9c3e-09d2b2fbdb41",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# combining variables\n",
    "env_plastock = work_datai.copy()\n",
    "\n",
    "\n",
    "# the substrat and distance features are being combined\n",
    "# the two lowest and the two highest of each group are being combined\n",
    "# substrat is a matter of combining different granularities. They are being grouped as\n",
    "# sand and gravel.\n",
    "# distance is now grouped by locations either less than or equal to 500 meters\n",
    "env_plastock.loc[env_plastock.substrat <= 2, 'substrat'] = 1\n",
    "env_plastock.loc[env_plastock.substrat > 2, 'substrat'] = 2\n",
    "env_plastock.loc[env_plastock.distance <= 2, 'distance'] = 1\n",
    "env_plastock.loc[env_plastock.distance > 2, 'distance'] = 2\n",
    "env_plastock.loc[env_plastock[\"fréquentation\"] <= 2, 'fréquentation'] = 2\n",
    "work_data_combined = env_plastock.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f2b4625-65e8-4ddc-9aa0-4e3f3d1fbdea",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "wk_dt =work_datai.groupby('échantillon', as_index=False).compte.sum()\n",
    "limit = np.quantile(wk_dt.compte.values, .99)\n",
    "not_these = wk_dt[wk_dt.compte > limit +1][\"échantillon\"].unique()\n",
    "wd_10 = work_datai[~work_datai[\"échantillon\"].isin(not_these)].copy()\n",
    "wd_10dt = wd_10.groupby('échantillon', as_index=False).compte.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e55d98-6c25-463f-85a1-279a94a6ed64",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "wk_dt =wd_10.groupby('échantillon', as_index=False).compte.sum()\n",
    "# fragments versus fibres\n",
    "# eliminate anything beyond the 99th percentile\n",
    "\n",
    "sit_disp = pd.DataFrame(translate_describe(wk_dt.compte.describe(), \"résultats\"))\n",
    "sit_disp.loc[\"échantillon\", \"résultats\"] = wd_10[\"échantillon\"].nunique()\n",
    "sit_disp.loc[\"total\", \"résultats\"] = wd_10.compte.sum()\n",
    "sit_disp[\"résultats\"] = sit_disp[\"résultats\"].astype(int)\n",
    "sit_disp = sit_disp.style.set_table_styles(table_css_styles).format(**psc.format_kwargs)\n",
    "sit_disp = add_table_to_page(sit_disp, 1, \"\", section, page, \"\")\n",
    "glue('sit_disp_micro', sit_disp, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d2dfa7-7eef-4a04-98d7-a22e3ae1bd60",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(8,8))\n",
    "wk_dt = wd_10.groupby('échantillon', as_index=False).particules.sum()\n",
    "vals = 'particules'\n",
    "object_column = vals\n",
    "ylim = np.quantile(wk_dt.particules.values, .99)\n",
    "xlim = np.quantile(wk_dt.particules.values, .99)\n",
    "sns.scatterplot(wk_dt, x=\"échantillon\", y=vals, label='Plastock micros particules', ax=axs[0, 0])\n",
    "sns.boxplot(wk_dt, y=vals,  showfliers=True, ax=axs[0, 1], dodge=False)\n",
    "sns.histplot(wk_dt, x=vals,  ax=axs[1, 0], stat='probability', kde=True)\n",
    "sns.ecdfplot(wk_dt, x=vals,  ax=axs[1, 1])\n",
    "\n",
    "# axs[0,0].xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "# axs[0,0].xaxis.set_major_formatter(mdates.DateFormatter('%m-%y'))\n",
    "\n",
    "axs[0, 0].set_ylim(-1, ylim)\n",
    "axs[0, 1].set_ylim(-1, ylim)\n",
    "axs[1, 1].set_xlim(-1, xlim)\n",
    "axs[1, 0].set_xlim(-1, xlim)\n",
    "# Hide X and Y axes tick marks\n",
    "axs[0,0].set_xticks([])\n",
    "\n",
    "\n",
    "axs[0, 0].set_xlabel(\"échantillon\")\n",
    "axs[0, 0].set_ylabel(\"Particules\")\n",
    "\n",
    "axs[1, 0].set_xlabel(\"Particules\")\n",
    "axs[1, 0].set_ylabel(\"Probabilité\")\n",
    "axs[0, 1].set_xlabel(\"\")\n",
    "axs[0, 1].set_ylabel(\"Particules\")\n",
    "axs[0, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "axs[1, 1].set_xlabel(\"Particules\")\n",
    "axs[0,0].set_title(\"Total par échantillon\", loc=\"left\")\n",
    "axs[0,1].set_title(\"Boîte de Tukey\", loc=\"left\")\n",
    "axs[1,0].set_title(\"Histogramme\", loc=\"left\")\n",
    "axs[1,1].set_title(\"Fonction de répartition\", loc=\"left\")\n",
    "plt.subplots_adjust(wspace=.3)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "glue('micro-situataion-sa', fig, display=False)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4480a701-2e29-455b-86a3-0abb9b84f20b",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# composition of total\n",
    "parts = work_data.groupby([\"objet\"], as_index=False).compte.sum()\n",
    "parts[\"% du total\"] = parts.compte/parts.compte.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b051a994-a36c-4f39-85a5-06c3f208bbc3",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "data = work_data[~work_data[\"échantillon\"].isin(not_these)].groupby(['échantillon', 'objet'], as_index=False).particules.sum()\n",
    "table2 = data.groupby('objet').particules.describe()\n",
    "table2 = table2.reindex(['fibres', 'fdure', 'souple'])\n",
    "table2 = table2.astype('int')\n",
    "\n",
    "\n",
    "table2['Particule'] = table2.index.map(lambda x: psc.particle_groups[x])\n",
    "caption = \"hello\"\n",
    "\n",
    "table2 = table2[[table2.columns[-1], *table2.columns[:-1]]].style.set_table_styles(table_css_styles).hide(axis=0)\n",
    "table2 = pst.add_table_to_page(table2, 2, caption, section, page, \"\", format_index=\"both\")\n",
    "glue('table2', table2, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "976be5d9-f3f3-4bcb-96ab-db8b7d0bb351",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "data = wd_10.groupby(['échantillon', 'objet'], as_index=False).particules.sum()\n",
    "table3 = data.groupby('objet').particules.describe()\n",
    "table3 = table3.astype('int')\n",
    "table3['Particule'] = [x.capitalize() for x in table3.index]\n",
    "\n",
    "table3 = table3[[table3.columns[-1], *table3.columns[:-1]]].style.set_table_styles(table_css_styles)\n",
    "caption = \"hello\"\n",
    "table3 = pst.add_table_to_page(table3, 3, caption, section, page, \"\", format_index='both')\n",
    "table3 = table3.hide(axis='index')\n",
    "glue('table3', table3, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e08e7937-1f56-4823-832d-b1311fddc2ee",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "obj = 'fibres'\n",
    "feature = 'substrat'\n",
    "\n",
    "cols = ['échantillon', feature,  'objet']\n",
    "\n",
    "data = work_data[~work_data[\"échantillon\"].isin(not_these)].copy()\n",
    "data = data[data.objet == obj].groupby(cols, as_index=False).particules.sum()\n",
    "table4 = data.groupby(feature).particules.describe()\n",
    "table4 = table4.astype('int')\n",
    "table4[feature] = table4.index.map(lambda x: name_substrate[x])\n",
    "table4.set_index(feature, inplace=True, drop=True)\n",
    "table4.index.name = None\n",
    "\n",
    "\n",
    "\n",
    "table4 = table4.style.set_table_styles(table_css_styles)\n",
    "caption = \"hello\"\n",
    "\n",
    "\n",
    "table4 = add_table_to_page(table4, 4, caption, section, page, \"\", format_index='both')\n",
    "glue('table4', table4, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68dce72-396f-4242-9a6d-125fd32f3616",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "obj = 'fibres'\n",
    "feature = 'fréquentation'\n",
    "\n",
    "cols = ['échantillon', feature,  'objet']\n",
    "\n",
    "data = work_data[~work_data[\"échantillon\"].isin(not_these)].copy()\n",
    "data = data[data.objet == obj].groupby(cols, as_index=False).particules.sum()\n",
    "table4 = data.groupby(feature).particules.describe()\n",
    "table4 = table4.astype('int')\n",
    "table4[feature] = table4.index.map(lambda x: name_frequentation[x])\n",
    "table4.set_index(feature, inplace=True, drop=True)\n",
    "table4.index.name = None\n",
    "\n",
    "\n",
    "\n",
    "table4 = table4.style.set_table_styles(table_css_styles)\n",
    "caption = \"hello\"\n",
    "\n",
    "\n",
    "table5 = add_table_to_page(table4, 5, caption, section, page, \"\", format_index='both')\n",
    "glue('table5', table5, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6be96dd1-3958-4b0a-9a2f-7ae4d7b14ce6",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "obj = 'fibres'\n",
    "feature = 'situation'\n",
    "\n",
    "cols = ['échantillon', feature,  'objet']\n",
    "\n",
    "data = work_data[~work_data[\"échantillon\"].isin(not_these)].copy()\n",
    "data = data[data.objet == obj].groupby(cols, as_index=False).particules.sum()\n",
    "table4 = data.groupby(feature).particules.describe()\n",
    "table4 = table4.astype('int')\n",
    "table4[feature] = table4.index.map(lambda x: name_situation[x])\n",
    "table4.set_index(feature, inplace=True, drop=True)\n",
    "table4.index.name = None\n",
    "\n",
    "\n",
    "\n",
    "table4 = table4.style.set_table_styles(table_css_styles)\n",
    "caption = \"hello\"\n",
    "\n",
    "\n",
    "table6 = add_table_to_page(table4, 6, caption, section, page, \"\", format_index='both')\n",
    "glue('table6', table6, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0e7074d-b775-4ab6-9392-fe812e35ce39",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "obj = 'fibres'\n",
    "feature = 'distance'\n",
    "\n",
    "cols = ['échantillon', feature,  'objet']\n",
    "\n",
    "data = work_data[~work_data[\"échantillon\"].isin(not_these)].copy()\n",
    "data = data[data.objet == obj].groupby(cols, as_index=False).particules.sum()\n",
    "table4 = data.groupby(feature).particules.describe()\n",
    "table4 = table4.astype('int')\n",
    "table4[feature] = table4.index.map(lambda x: name_distance[x])\n",
    "table4.set_index(feature, inplace=True, drop=True)\n",
    "table4.index.name = None\n",
    "\n",
    "\n",
    "\n",
    "table4 = table4.style.set_table_styles(table_css_styles)\n",
    "caption = \"hello\"\n",
    "\n",
    "\n",
    "table7 = add_table_to_page(table4, 4, caption, section, page, \"\", format_index='both')\n",
    "glue('table7', table7, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce92188c-a5b0-43ca-860a-c29086319fbe",
   "metadata": {},
   "source": [
    "# Micros particules \n",
    "\n",
    "__Format__ \n",
    "\n",
    "Le format suit celui de l'annexe pour les [microplastiques](micro_atts) . Nous incluons également l'analyse utilisant des variables combinées selon la méthode décrite dans la section [Résultats précédents](previous_results).\n",
    "\n",
    "__Le système de mesure.__\n",
    "\n",
    "Dans cette section, les unités sont le nombre de particules par conteneur. Le récipient a des côtés de 10 cm X 10 cm X 5 cm ou 0,0005 m³.\n",
    "\n",
    "## Résultats\n",
    "\n",
    "::::{grid} 1 1 2 2\n",
    "\n",
    ":::{grid-item}\n",
    "\n",
    "{glue}`sit_disp_micro`\n",
    "\n",
    ":::\n",
    "\n",
    ":::{grid-item}\n",
    "\n",
    "From table [A1-4](micro-table-A1-4), the maximum count was 2'991 particles. This was recorded at la Pichette, where 2'593 fibers were counted in one sample. The 99th percentile of all the sample totals is 1060 particles per sample. This was exceeded three times, twice at Port Choiseul and once at la Pichette.\n",
    "\n",
    "Here we consider the data limited to the 99th percentile, that is we are eliminating the three samples that exceeded 1061 particles.\n",
    "\n",
    "<b>Table MP-1 :</b> Distribution des résultats de l’ensemble de l’échantillons Plastock. All values less than 1061 (90th percentile).\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    ":::{card} \n",
    "\n",
    "{glue}`micro-situataion-sa`\n",
    "\n",
    "+++\n",
    "<b> Figure 2.2 :</b> Plastock 2022: distribution des résultats de l’ensemble de l’échantillons Plastock. The elimination of those three samples resulted in a 24 point drop in the average and a 115 point drop in the standard deviation. \n",
    ":::\n",
    "\n",
    "\n",
    "(conditions_mp)=\n",
    "### Conditions d'échantillonage\n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Resumé\n",
    ":selected:\n",
    "\n",
    "{glue}`table2`\n",
    "\n",
    "{glue}`table3`\n",
    "\n",
    "\n",
    "::: \n",
    "\n",
    ":::{tab-item} Fibres\n",
    "\n",
    "{glue}`table4`\n",
    "\n",
    "{glue}`table5`\n",
    "\n",
    "{glue}`table6`\n",
    "\n",
    "{glue}`table7`\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Fragments\n",
    "\n",
    "frags\n",
    "::: \n",
    "\n",
    ":::{tab-item} Souples\n",
    "\n",
    "suple\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a9c6a06-1afc-4bfa-862f-e2b1da4ec0fc",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def analyze_scenario(scenario_data, func, n_iterations=100, bin_width=0.2):\n",
    "    \"\"\"\n",
    "    Analyze a specific scenario using Random Forest regression with bootstrapping,\n",
    "    and calculate feature importances.\n",
    "\n",
    "    :param data: DataFrame containing the dataset.\n",
    "    :param feature_1: The name of the first feature for filtering.\n",
    "    :param feature_1_value: The value of the first feature to filter by.\n",
    "    :param feature_2: The name of the second feature for filtering.\n",
    "    :param feature_2_value: The value of the second feature to filter by.\n",
    "    :param n_iterations: Number of bootstrap iterations. Default is 100.\n",
    "    :param bin_width: Width of each bin for histogram. Default is 0.2.\n",
    "    :return: A tuple containing bins, bin probabilities, flattened predictions, and feature importances.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare data for regression\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaled = y_scaler.fit_transform(scenario_data['pcs_m'].values.reshape(-1,1)).flatten()\n",
    "    \n",
    "    # Initialize the OneHotEncoder\n",
    "    # here we encode the ordinal data\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    \n",
    "    X = scenario_data.drop('pcs_m', axis=1)\n",
    "    \n",
    "    # Apply the encoder to the categorical columns\n",
    "    encoded_data = encoder.fit_transform(scenario_data[['fréquentation', 'situation', 'distance', 'substrat']])\n",
    "    # Create a DataFrame with the encoded data\n",
    "    X_encoded = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['fréquentation', 'situation', 'distance', 'substrat']))\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_scaled, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Bootstrap predictions and accumulate feature importances\n",
    "    bootstrap_predictions = []\n",
    "        \n",
    "    # Collect diagnostic at each repetition\n",
    "    cum_mse = []\n",
    "    cum_r2 = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        X_train_sample, y_train_sample = resample(X_train, y_train, n_samples= len(y_train) -1)\n",
    "        rf_model_sample = func\n",
    "        rf_model_sample.fit(X_train_sample, y_train_sample)\n",
    "        \n",
    "        pred = rf_model_sample.predict(X_test)\n",
    "        \n",
    "        r2 = r2_score(y_test, pred)\n",
    "        pred = y_scaler.inverse_transform(pred.reshape(-1, 1)).flatten()\n",
    "        bootstrap_predictions.append(pred)\n",
    "        mse = mean_squared_error(y_test , pred)\n",
    "        \n",
    "        cum_mse.append(mse)\n",
    "        cum_r2.append(r2)           \n",
    "\n",
    "    # Flatten the predictions array\n",
    "    predictions_flat = np.array(bootstrap_predictions).flatten()\n",
    "    \n",
    "    return predictions_flat, cum_mse, cum_r2\n",
    "\n",
    "def plot_histogram(predictions, observed, title=\"\", reference='camp-dist-1', display=False, order='predictions', xmax=800, n_bins=20, x_label='pcs/m³'):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    if order == 'predictions':\n",
    "        sns.histplot(predictions, binwidth=n_bins, stat=\"probability\", ax=ax, label='prédictions', zorder=0)\n",
    "        sns.histplot(observed, binwidth=n_bins, stat=\"probability\", label='observée', zorder=1, ax=ax)\n",
    "    else:\n",
    "        sns.histplot(predictions, binwidth=n_bins, stat=\"probability\", ax=ax, label='prédictions', zorder=1)\n",
    "        sns.histplot(observed, binwidth=n_bins, stat=\"probability\", label='observée', zorder=0, ax=ax)\n",
    "    ax.set_xlim(-.1, xmax)\n",
    "    plt.title(title, loc='left')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Densité de Probabilité')\n",
    "    plt.legend()\n",
    "    glue(reference, fig, display=display)\n",
    "    plt.close()\n",
    "\n",
    "def evalutate_model(r2s, mses, label, model='random-forest'):\n",
    "    r2 = np.round(np.mean(r2s), 2)\n",
    "    mse = np.round(np.mean(mses), 2)\n",
    "    results = {\"cross validated error\":r2, \"mean² error\":mse, 'model':model}\n",
    "    return pd.DataFrame(results, index=[label])\n",
    "\n",
    "# Calculating quantiles for Scenario 2\n",
    "format_kwargs = dict(precision=0, thousands=\"'\")\n",
    "q_uants = [0.01, 0.25, 0.5, 0.75, 0.99]\n",
    "index = ['1%', '25%', '50%', '75%', '99%', 'Moyenne']\n",
    "def makeqdf(observed, predicted, index=index, quants=q_uants, caption=\"\"):\n",
    "    \n",
    "    o_q = np.quantile(observed, quants)\n",
    "    m_o = np.mean(observed)\n",
    "    o_p = np.quantile(predicted, quants)\n",
    "    m_p = np.mean(predicted)\n",
    "    \n",
    "    results = {'observée':[*o_q, m_o], 'prédiction': [*o_p, m_p]}\n",
    "   \n",
    "    return pd.DataFrame(results, index=index).style.set_table_styles(table_css_styles_top).format(**format_kwargs).set_caption(caption)\n",
    "\n",
    "cols = ['échantillon', 'position', 'fréquentation','situation', 'distance', 'substrat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c91f3523-e232-4c59-b7fe-a328b70640af",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "estimators = 10\n",
    "iterations = 10\n",
    "\n",
    "test_xt = work_data_combined.copy()\n",
    "# the volume of the container used to sample in cm³\n",
    "# sample_volume_cm = 10*10*5\n",
    "# sample_volume_m = sample_volume_cm/10**6\n",
    "test_xt['pcs_m'] = test_xt['compte']\n",
    "\n",
    "\n",
    "# Filter for Scenario \n",
    "test_xi = test_xt[(test_xt['position'] == 1)].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat',  'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2 = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Ligne d\\'eau'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-lignedeau-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Ligne d\\'eau'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='lignedeau-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94ff68ec-50b3-4cad-9ff0-e7c81e9617c6",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "\n",
    "test_xt = work_data_combined.copy()\n",
    "# the volume of the container used to sample in cm³\n",
    "# sample_volume_cm = 10*10*5\n",
    "# sample_volume_m = sample_volume_cm/10**6\n",
    "test_xt['pcs_m'] = test_xt['compte']\n",
    "\n",
    "\n",
    "# Filter for Scenario \n",
    "test_xi = test_xt.copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat',  'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions,mse, r2  = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Tous les positions'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-tous-md-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='tous-md-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94bb0f64-eaab-4b39-a648-938aa42a16e8",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "test_xi = test_xt[(test_xt['position'] == 2)].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat', 'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2 = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Plage seche'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-plageseche-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Plage seche'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='plageseche-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e78b009-f725-40d2-a9d5-5906eeb572cc",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "cols = ['échantillon', 'position', 'fréquentation','situation', 'distance', 'substrat']\n",
    "test_xi = test_xt[(test_xt['position'] == 2)&(test_xt.objet == 'fibres')].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat', 'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2 = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Plage seche et fibres'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-plagesechefibres-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Plage seche et fibres'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='plagesechefibres-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a91d8da-3067-4c5a-986e-9daa8bdc4ba4",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "test_xi = test_xt[(test_xt['substrat'] == 1)].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat', 'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2 = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Sables fins'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-sablesfins-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Sables fins'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='sablesfins-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d98ec6b-0f31-4d80-8b94-ac5c73792dfd",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "test_xi = test_xt[(test_xt['substrat'] == 2)].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat', 'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2 = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Cailloux'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-cailloux-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Cailloux'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='cailloux-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "625968dd-abe1-4da3-88d6-7df8e376d3ec",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "test_xi = test_xt[(test_xt['fréquentation'] == 3)].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat', 'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2 = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Fréquentation élevée'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-freq3-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Fréquentation élevée'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='freq3-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "790d9487-2da8-4330-b7b6-6488edb5c367",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "test_xi = test_xt[(test_xt['fréquentation'] == 2)].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat', 'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2  = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Fréquentation moyenne'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-freq2-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Sables fins'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='freq2-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089cbb88-656a-4948-88da-ece0ac79282d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63dfdd4b-eaa3-408b-99a7-5ef417a215ee",
   "metadata": {},
   "source": [
    "(random_forest_sa_md)=\n",
    "### Random Forest \n",
    "\n",
    "Source : [scikit-learn random forest](https://scikit-learn.org/0.16/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "\n",
    "criterion : `absolute error`\n",
    "\n",
    "La régression avec forêt aléatoire est une technique d'apprentissage automatique (machine learning) utilisée pour prédire des résultats continus (par opposition aux catégories dans la classification). C'est une méthode d'apprentissage ensembliste, ce qui signifie qu'elle combine les prédictions de plusieurs algorithmes d'apprentissage automatique pour produire des prédictions plus précises.\n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Toutes les conditions\n",
    "{glue}`tous-md-sa`\n",
    ":::\n",
    "\n",
    ":::{tab-item} Fréquentation moyenne\n",
    "{glue}`freq2-sa`\n",
    ":::\n",
    "\n",
    ":::{tab-item} Haute fréquentation\n",
    "{glue}`freq3-sa`\n",
    ":::\n",
    "\n",
    ":::{tab-item} Cailloux\n",
    "{glue}`cailloux-sa`\n",
    ":::\n",
    "\n",
    ":::{tab-item} Sables fins\n",
    "{glue}`sablesfins-sa`\n",
    ":::\n",
    "\n",
    ":::{tab-item} Plage seche et sable fins\n",
    "{glue}`plagesechefibres-sa`\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Plage seche\n",
    "{glue}`plageseche-sa`\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Ligne d'eau\n",
    "{glue}`lignedeau-sa`\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    ":::{tab-item} Résultats\n",
    ":selected:\n",
    "\n",
    "````{grid} 1 2 2 2\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-tous-md-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "\n",
    "Les modèles ont fait l'objet d'un bootstrap, 100 itérations pour chaque scénario. Les résultats estimés sont la collection de toutes les prédictions de chaque itération.\n",
    "\n",
    "Par exemple, le tableau intitulé \"Gravier\" présente les résultats observés et prévus pour les plages ayant un substrat de 3 ou 4.\n",
    "\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-freq2-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-freq3-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-cailloux-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-sablesfins-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-plagesechefibres-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-plageseche-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-lignedeau-sa`\n",
    "```\n",
    "\n",
    "````\n",
    ":::\n",
    "\n",
    "::::\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e50a7c8a-1316-4170-88c2-d70d1f64c3ef",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git repo: https://github.com/hammerdirt-analyst/plastock.git\n",
      "\n",
      "Git branch: main\n",
      "\n",
      "matplotlib: 3.7.1\n",
      "seaborn   : 0.12.2\n",
      "numpy     : 1.24.2\n",
      "pandas    : 2.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions -b -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba80e4d-6655-47b7-91fd-6276e80722b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}