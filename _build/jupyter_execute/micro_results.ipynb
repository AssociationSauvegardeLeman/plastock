{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32bcdb7d-96d5-47f5-a77a-139bafcc6a7d",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from myst_nb import glue\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "\n",
    "from plastockconf import name_zones, name_particles, name_frequentation, name_situation\n",
    "from plastockconf import particle_groups, name_substrate, name_distance, table_css_styles, table_css_styles_top\n",
    "\n",
    "from plastock import attribute_summary, attribute_summary_test, attribute_summary_grid, add_table_to_page\n",
    "\n",
    "a_property =  {'color' : 'red'}\n",
    "format_kwargs = dict(precision=2, thousands=\"'\", decimal=\",\")\n",
    "glue('blank_caption', \" \", display=False)\n",
    "\n",
    "section = 'A'\n",
    "page = 2\n",
    "\n",
    "work_data = pd.read_csv(\"data/end_pipe/long_form_micro.csv\")\n",
    "work_data.rename(columns={'echantillon':'échantillon', 'frequentation':'fréquentation'}, inplace=True)\n",
    "beach_data = pd.read_csv(\"data/end_pipe/asl_beaches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beda0b95-bf7b-4173-b9e3-4ae226a3f2b5",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "table_no = 1\n",
    "figure_no = 1\n",
    "\n",
    "caption = 'Les données d\\'analyse. Tous les tableaux et figures sont construits à partir de cet ensemble de données.'\n",
    "rule = 'Les attributs dont la moyenne des résultats est supérieure à la moyenne du projet sont en rouge.'\n",
    "\n",
    "t_0 = work_data.head().style.set_table_styles(table_css_styles)\n",
    "table_0 = add_table_to_page(t_0, table_no, caption, section, page, rule, format_index='columns')\n",
    "glue('tablea21', table_0, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006b1cf8-46b5-4359-ab2e-d5c0233efead",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ! combine souples et dur !\n",
    "fibers = work_data.loc[work_data.objet == 'fibres'].copy()\n",
    "not_fibers = work_data.loc[work_data.objet != 'fibres'].copy()\n",
    "\n",
    "not_fibers['objet'] = 'fragments'\n",
    "\n",
    "work_datai = pd.concat([fibers, not_fibers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31df0293-bb2f-4aa2-9c3e-09d2b2fbdb41",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "def name_the_new_distance(x, less='<= 500 m', more = '> 500 m'):\n",
    "    if x == 1:\n",
    "        return less\n",
    "    else:\n",
    "        return more\n",
    "\n",
    "def name_the_new_freq(x, new):\n",
    "    if x <= 2:\n",
    "        return new\n",
    "    else:\n",
    "        return 'Elévée'\n",
    "\n",
    "\n",
    "# the feature variables are added to the work_data\n",
    "ti = work_datai.copy()\n",
    "features = ['frequentation', 'situation', 'orientation', 'distance']\n",
    "\n",
    "beach_datax = pd.read_csv(\"data/end_pipe/asl_beaches.csv\").set_index('Plage')\n",
    "\n",
    "# they can be merged on the Plage column and the index\n",
    "env_plastock = work_datai.copy()\n",
    "\n",
    "# # ! creation of composite variables !\n",
    "# t_and_f = env_plastock.loc[:, ['échantillon', 'slug','date','code', 'pcs/m²', 'quantité', 'frequentation', 'situation', 'distance', 'substrat']].copy()\n",
    "# env_plastock\n",
    "\n",
    "# the substrat and distance features are being combined\n",
    "# the two lowest and the two highest of each group are being combined\n",
    "# substrat is a matter of combining different granularities. They are being grouped as\n",
    "# sand and gravel.\n",
    "# distance is now grouped by locations either less than or equal to 500 meters\n",
    "env_plastock.loc[env_plastock.substrat <= 2, 'substrat'] = 1\n",
    "env_plastock.loc[env_plastock.substrat > 2, 'substrat'] = 2\n",
    "env_plastock.loc[env_plastock.distance <= 2, 'distance'] = 1\n",
    "env_plastock.loc[env_plastock.distance > 2, 'distance'] = 2\n",
    "env_plastock.loc[env_plastock[\"fréquentation\"] <= 2, 'fréquentation'] = 2\n",
    "work_datai = env_plastock.copy()\n",
    "\n",
    "\n",
    "regions = pd.read_csv(\"data/end_pipe/lac_leman_regions.csv\")\n",
    "regions.loc[regions.slug == 'savoniere', 'slug'] = 'savonniere'\n",
    "regions.drop_duplicates('slug', inplace=True)\n",
    "regions.set_index('slug', drop=True, inplace=True)\n",
    "from slugify import slugify\n",
    "work_datai['slug'] = work_datai.Plage.apply(lambda x: slugify(x))\n",
    "work_datai['region'] = work_datai.slug.apply(lambda x: regions.loc[x, 'alabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e707e242-1947-44b6-a2e3-bd19863e06b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Plage</th>\n",
       "      <th>échantillon</th>\n",
       "      <th>orientation</th>\n",
       "      <th>position</th>\n",
       "      <th>substrat</th>\n",
       "      <th>fréquentation</th>\n",
       "      <th>situation</th>\n",
       "      <th>distance</th>\n",
       "      <th>objet</th>\n",
       "      <th>compte</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amphion</td>\n",
       "      <td>74_Amp_1</td>\n",
       "      <td>NE</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fibres</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amphion</td>\n",
       "      <td>74_Amp_10</td>\n",
       "      <td>NNE</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fibres</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amphion</td>\n",
       "      <td>74_Amp_2</td>\n",
       "      <td>NNE</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fibres</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amphion</td>\n",
       "      <td>74_Amp_3</td>\n",
       "      <td>NE</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fibres</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amphion</td>\n",
       "      <td>74_Amp_4</td>\n",
       "      <td>NNE</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fibres</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Plage échantillon orientation  position  substrat  fréquentation   \n",
       "0  Amphion    74_Amp_1          NE         1         4              3  \\\n",
       "1  Amphion   74_Amp_10         NNE         2         4              3   \n",
       "2  Amphion    74_Amp_2         NNE         1         4              3   \n",
       "3  Amphion    74_Amp_3          NE         1         4              3   \n",
       "4  Amphion    74_Amp_4         NNE         1         4              3   \n",
       "\n",
       "   situation  distance   objet  compte  \n",
       "0          1         1  fibres      97  \n",
       "1          1         1  fibres     140  \n",
       "2          1         1  fibres     121  \n",
       "3          1         1  fibres      31  \n",
       "4          1         1  fibres     179  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b2946f4-0c9b-411e-a135-7193bc5a7201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Plage</th>\n",
       "      <th>échantillon</th>\n",
       "      <th>orientation</th>\n",
       "      <th>position</th>\n",
       "      <th>substrat</th>\n",
       "      <th>fréquentation</th>\n",
       "      <th>situation</th>\n",
       "      <th>distance</th>\n",
       "      <th>objet</th>\n",
       "      <th>compte</th>\n",
       "      <th>slug</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amphion</td>\n",
       "      <td>74_Amp_1</td>\n",
       "      <td>NE</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fibres</td>\n",
       "      <td>97</td>\n",
       "      <td>amphion</td>\n",
       "      <td>Grand lac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amphion</td>\n",
       "      <td>74_Amp_10</td>\n",
       "      <td>NNE</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fibres</td>\n",
       "      <td>140</td>\n",
       "      <td>amphion</td>\n",
       "      <td>Grand lac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amphion</td>\n",
       "      <td>74_Amp_2</td>\n",
       "      <td>NNE</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fibres</td>\n",
       "      <td>121</td>\n",
       "      <td>amphion</td>\n",
       "      <td>Grand lac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amphion</td>\n",
       "      <td>74_Amp_3</td>\n",
       "      <td>NE</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fibres</td>\n",
       "      <td>31</td>\n",
       "      <td>amphion</td>\n",
       "      <td>Grand lac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amphion</td>\n",
       "      <td>74_Amp_4</td>\n",
       "      <td>NNE</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>fibres</td>\n",
       "      <td>179</td>\n",
       "      <td>amphion</td>\n",
       "      <td>Grand lac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Plage échantillon orientation  position  substrat  fréquentation   \n",
       "0  Amphion    74_Amp_1          NE         1         2              3  \\\n",
       "1  Amphion   74_Amp_10         NNE         2         2              3   \n",
       "2  Amphion    74_Amp_2         NNE         1         2              3   \n",
       "3  Amphion    74_Amp_3          NE         1         2              3   \n",
       "4  Amphion    74_Amp_4         NNE         1         2              3   \n",
       "\n",
       "   situation  distance   objet  compte     slug     region  \n",
       "0          1         1  fibres      97  amphion  Grand lac  \n",
       "1          1         1  fibres     140  amphion  Grand lac  \n",
       "2          1         1  fibres     121  amphion  Grand lac  \n",
       "3          1         1  fibres      31  amphion  Grand lac  \n",
       "4          1         1  fibres     179  amphion  Grand lac  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_datai.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad8e21ad-789e-4a06-88ac-89e95cbc7b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alabel</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slug</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anarchy-beach</th>\n",
       "      <td>Haut lac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arabie</th>\n",
       "      <td>Haut lac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bain-des-dames</th>\n",
       "      <td>Haut lac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baye-de-clarens</th>\n",
       "      <td>Haut lac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baye-de-montreux-d</th>\n",
       "      <td>Haut lac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      alabel\n",
       "slug                        \n",
       "anarchy-beach       Haut lac\n",
       "arabie              Haut lac\n",
       "bain-des-dames      Haut lac\n",
       "baye-de-clarens     Haut lac\n",
       "baye-de-montreux-d  Haut lac"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b525da4f-ecf0-4bf5-a7ee-878f40ae9868",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# def name_the_new_distance(x, less='<= 500 m', more = '> 500 m'):\n",
    "#     if x == 1:\n",
    "#         return less\n",
    "#     else:\n",
    "#         return more\n",
    "\n",
    "# def name_the_new_freq(x, new):\n",
    "#     if x <= 2:\n",
    "#         return new\n",
    "#     else:\n",
    "#         return 'Elévée'\n",
    "\n",
    "\n",
    "# # the feature variables are added to the work_data\n",
    "# ti = work_data.copy()\n",
    "# features = ['frequentation', 'situation', 'orientation', 'distance']\n",
    "\n",
    "# beach_datax = pd.read_csv(\"data/end_pipe/asl_beaches.csv\").set_index('Plage')\n",
    "\n",
    "# # they can be merged on the Plage column and the index\n",
    "# env_plastock = ti.merge(beach_datax[features], left_on='Plage', right_index=True)\n",
    "\n",
    "# # ! creation of composite variables !\n",
    "# t_and_f = env_plastock.loc[:, ['échantillon', 'slug','date','code', 'pcs/m²', 'quantité', 'frequentation', 'situation', 'distance', 'substrat']].copy()\n",
    "\n",
    "\n",
    "# # the substrat and distance features are being combined\n",
    "# # the two lowest and the two highest of each group are being combined\n",
    "# # substrat is a matter of combining different granularities. They are being grouped as\n",
    "# # sand and gravel.\n",
    "# # distance is now grouped by locations either less than or equal to 500 meters\n",
    "# t_and_f.loc[t_and_f.substrat <= 2, 'substrat'] = 1\n",
    "# t_and_f.loc[t_and_f.substrat > 2, 'substrat'] = 2\n",
    "# t_and_f.loc[t_and_f.distance <= 2, 'distance'] = 1\n",
    "# t_and_f.loc[t_and_f.distance > 2, 'distance'] = 2\n",
    "# t_and_f.loc[t_and_f.frequentation <= 2, 'frequentation'] = 2\n",
    "\n",
    "# # ! the data used in the models !\n",
    "# f_combi = t_and_f.copy()\n",
    "\n",
    "\n",
    "\n",
    "# f_combi.rename(columns={'frequentation':'fréquentation', 'loc_date': 'échantillon'}, inplace=True)\n",
    "\n",
    "# # the feature variables are combined along the ordinal axis. Going from four catgories\n",
    "# # to two in the case of distance and substrate. city and country are already binary\n",
    "# # the values of low and moderate frequentation are combined also.\n",
    "# f_comb = f_combi.copy()\n",
    "# f_comb['distance'] = f_comb['distance'].apply(lambda x: name_the_new_distance(x))\n",
    "# f_comb['fréquentation'] = f_comb['fréquentation'].apply(lambda x: name_the_new_freq(x, 'faible-moyenne'))\n",
    "# f_comb['situation'] = f_comb['situation'].apply(lambda x: name_situation[x])\n",
    "# f_comb['substrat'] = f_comb['substrat'].apply(lambda x: name_the_new_distance(x, less='Sable', more='Graviers'))\n",
    "\n",
    "\n",
    "# # ! no composite variables !\n",
    "# no_combined = env_plastock.loc[:, ['échantillon', 'slug','date','code', 'pcs/m²', 'frequentation', 'situation', 'distance', 'substrat']].copy()\n",
    "# no_combined.rename(columns={'frequentation':'fréquentation', 'loc_date': 'échantillon'}, inplace=True)\n",
    "\n",
    "# no_combined['distance'] = no_combined['distance'].apply(lambda x: name_distance[x])\n",
    "# no_combined['fréquentation'] = no_combined['fréquentation'].apply(lambda x: name_frequentation[x])\n",
    "# no_combined['situation'] = no_combined['situation'].apply(lambda x: name_situation[x])\n",
    "# no_combined['substrat'] = no_combined['substrat'].apply(lambda x: name_substrate[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9c6a06-1afc-4bfa-862f-e2b1da4ec0fc",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "def analyze_scenario(scenario_data, func, n_iterations=100, bin_width=0.2):\n",
    "    \"\"\"\n",
    "    Analyze a specific scenario using Random Forest regression with bootstrapping,\n",
    "    and calculate feature importances.\n",
    "\n",
    "    :param data: DataFrame containing the dataset.\n",
    "    :param feature_1: The name of the first feature for filtering.\n",
    "    :param feature_1_value: The value of the first feature to filter by.\n",
    "    :param feature_2: The name of the second feature for filtering.\n",
    "    :param feature_2_value: The value of the second feature to filter by.\n",
    "    :param n_iterations: Number of bootstrap iterations. Default is 100.\n",
    "    :param bin_width: Width of each bin for histogram. Default is 0.2.\n",
    "    :return: A tuple containing bins, bin probabilities, flattened predictions, and feature importances.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare data for regression\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaled = y_scaler.fit_transform(scenario_data['pcs_m'].values.reshape(-1,1)).flatten()\n",
    "    \n",
    "    # Initialize the OneHotEncoder\n",
    "    # here we encode the ordinal data\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    \n",
    "    X = scenario_data.drop('pcs_m', axis=1)\n",
    "    \n",
    "    # Apply the encoder to the categorical columns\n",
    "    encoded_data = encoder.fit_transform(scenario_data[['fréquentation', 'situation', 'distance', 'substrat']])\n",
    "    # Create a DataFrame with the encoded data\n",
    "    X_encoded = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['fréquentation', 'situation', 'distance', 'substrat']))\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_scaled, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Bootstrap predictions and accumulate feature importances\n",
    "    bootstrap_predictions = []\n",
    "        \n",
    "    # Collect diagnostic at each repetition\n",
    "    cum_mse = []\n",
    "    cum_r2 = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        X_train_sample, y_train_sample = resample(X_train, y_train, n_samples= len(y_train) -1)\n",
    "        rf_model_sample = func\n",
    "        rf_model_sample.fit(X_train_sample, y_train_sample)\n",
    "        \n",
    "        pred = rf_model_sample.predict(X_test)\n",
    "        \n",
    "        r2 = r2_score(y_test, pred)\n",
    "        pred = y_scaler.inverse_transform(pred.reshape(-1, 1)).flatten()\n",
    "        bootstrap_predictions.append(pred)\n",
    "        mse = mean_squared_error(y_test , pred)\n",
    "        \n",
    "        cum_mse.append(mse)\n",
    "        cum_r2.append(r2)           \n",
    "\n",
    "    # Flatten the predictions array\n",
    "    predictions_flat = np.array(bootstrap_predictions).flatten()\n",
    "    \n",
    "    return predictions_flat, cum_mse, cum_r2\n",
    "\n",
    "def plot_histogram(predictions, observed, title=\"\", reference='camp-dist-1', display=False, order='predictions', xmax=800, n_bins=20, x_label='pcs/m³'):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    if order == 'predictions':\n",
    "        sns.histplot(predictions, binwidth=n_bins, stat=\"probability\", ax=ax, label='prédictions', zorder=0)\n",
    "        sns.histplot(observed, binwidth=n_bins, stat=\"probability\", label='observée', zorder=1, ax=ax)\n",
    "    else:\n",
    "        sns.histplot(predictions, binwidth=n_bins, stat=\"probability\", ax=ax, label='prédictions', zorder=1)\n",
    "        sns.histplot(observed, binwidth=n_bins, stat=\"probability\", label='observée', zorder=0, ax=ax)\n",
    "    ax.set_xlim(-.1, xmax)\n",
    "    plt.title(title, loc='left')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Densité de Probabilité')\n",
    "    plt.legend()\n",
    "    glue(reference, fig, display=display)\n",
    "    plt.close()\n",
    "\n",
    "def evalutate_model(r2s, mses, label, model='random-forest'):\n",
    "    r2 = np.round(np.mean(r2s), 2)\n",
    "    mse = np.round(np.mean(mses), 2)\n",
    "    results = {\"cross validated error\":r2, \"mean² error\":mse, 'model':model}\n",
    "    return pd.DataFrame(results, index=[label])\n",
    "\n",
    "# Calculating quantiles for Scenario 2\n",
    "format_kwargs = dict(precision=0, thousands=\"'\")\n",
    "q_uants = [0.01, 0.25, 0.5, 0.75, 0.99]\n",
    "index = ['1%', '25%', '50%', '75%', '99%', 'Moyenne']\n",
    "def makeqdf(observed, predicted, index=index, quants=q_uants, caption=\"\"):\n",
    "    \n",
    "    o_q = np.quantile(observed, quants)\n",
    "    m_o = np.mean(observed)\n",
    "    o_p = np.quantile(predicted, quants)\n",
    "    m_p = np.mean(predicted)\n",
    "    \n",
    "    results = {'observée':[*o_q, m_o], 'prédiction': [*o_p, m_p]}\n",
    "   \n",
    "    return pd.DataFrame(results, index=index).style.set_table_styles(table_css_styles_top).format(**format_kwargs).set_caption(caption)\n",
    "\n",
    "cols = ['échantillon', 'position', 'fréquentation','situation', 'distance', 'substrat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c91f3523-e232-4c59-b7fe-a328b70640af",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "estimators = 10\n",
    "iterations = 100\n",
    "\n",
    "test_xt = work_datai.copy()\n",
    "# the volume of the container used to sample in cm³\n",
    "# sample_volume_cm = 10*10*5\n",
    "# sample_volume_m = sample_volume_cm/10**6\n",
    "test_xt['pcs_m'] = test_xt['compte']\n",
    "\n",
    "\n",
    "# Filter for Scenario \n",
    "test_xi = test_xt[(test_xt['position'] == 1)].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat',  'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2 = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Ligne d\\'eau'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-lignedeau-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Ligne d\\'eau'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='lignedeau-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d07b3ae5-e2f8-4b90-bd38-e3a749e5377e",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import spearmanr\n",
    "from collections import defaultdict\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def plot_permutation_importance(clf, X, y, ax):\n",
    "    result = permutation_importance(clf, X, y, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "    ax.boxplot(\n",
    "        result.importances[perm_sorted_idx].T,\n",
    "        vert=False,\n",
    "        labels=X.columns[perm_sorted_idx],\n",
    "    )\n",
    "    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "    return ax\n",
    "\n",
    "test_xt = work_datai.copy()\n",
    "# the sides of the object used to sample\n",
    "test_xt['pcs_m'] = test_xt['compte']\n",
    "\n",
    "#  # Prepare data for regression\n",
    "# scenario_data = test_xt.copy()\n",
    "# scenario_data.reset_index(inplace=True, drop=True)\n",
    "# y_scaler = MinMaxScaler()\n",
    "# y_scaled = y_scaler.fit_transform(scenario_data['pcs_m'].values.reshape(-1,1)).flatten()\n",
    "\n",
    "# # Initialize the OneHotEncoder\n",
    "# # here we encode the ordinal data\n",
    "# encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# X = scenario_data.drop('pcs_m', axis=1)\n",
    "# print(X.columns)\n",
    "\n",
    "# # Apply the encoder to the categorical columns\n",
    "# # encoded_data = encoder.fit_transform(scenario_data[['fréquentation', 'distance', 'substrat']])\n",
    "# # Create a DataFrame with the encoded data\n",
    "# # X_encoded = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['fréquentation', 'distance', 'substrat']))\n",
    "# # X_encoded[\"Plage\"] = scenario_data.Plage\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "# corr = spearmanr(X).correlation\n",
    "\n",
    "# # Ensure the correlation matrix is symmetric\n",
    "# corr = (corr + corr.T) / 2\n",
    "# np.fill_diagonal(corr, 1)\n",
    "\n",
    "# # We convert the correlation matrix to a distance matrix before performing\n",
    "# # hierarchical clustering using Ward's linkage.\n",
    "# distance_matrix = 1 - np.abs(corr)\n",
    "# dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "# dendro = hierarchy.dendrogram(\n",
    "#     dist_linkage, labels=X.columns.to_list(), ax=ax1, leaf_rotation=90\n",
    "# )\n",
    "# dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "# ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "# ax2.set_xticks(dendro_idx)\n",
    "# ax2.set_yticks(dendro_idx)\n",
    "# ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "# ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "# _ = fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4431f4f6-b07f-4b0b-bb78-b10c1ac7c6a5",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# cluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "# cluster_id_to_feature_ids = defaultdict(list)\n",
    "# for idx, cluster_id in enumerate(cluster_ids):\n",
    "#     cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "# selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "# selected_features_names = X.columns[selected_features]\n",
    "# print(selected_features_names)\n",
    "\n",
    "# X_train_sel = X_train[selected_features_names]\n",
    "# X_test_sel = X_test[selected_features_names]\n",
    "\n",
    "# clf_sel = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "# clf_sel.fit(X_train_sel, y_train)\n",
    "# print(\n",
    "#     \"Baseline accuracy on test data with features removed:\"\n",
    "#     f\" {clf_sel.score(X_test_sel, y_test):.2}\"\n",
    "# )\n",
    "\n",
    "# selected_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cad48652-985d-44ab-83b5-738110dac489",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(7, 6))\n",
    "# ax = plot_permutation_importance(clf_sel, X_test_sel, y_test, ax)\n",
    "# ax.set_title(\"Permutation Importances on selected subset of features\\n(test set)\")\n",
    "# ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27d2a50b-ddd5-4d9e-b366-aecdeaaf9a87",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# feat_importances = pd.DataFrame(feature_importance, index=imp_cols, columns=[\"Importance\"])\n",
    "\n",
    "# feat_importances = feat_importances.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# sns.barplot(feat_importances, y=feat_importances.index, x=\"Importance\", ax=ax)\n",
    "# ax.tick_params(which='both', axis='x', labelrotation=45)\n",
    "# ax.set_title(\"Importance des variables, postion = Ligne d'eau\", loc='left')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('variable_importance.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94ff68ec-50b3-4cad-9ff0-e7c81e9617c6",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "\n",
    "test_xt = work_datai.copy()\n",
    "# the volume of the container used to sample in cm³\n",
    "# sample_volume_cm = 10*10*5\n",
    "# sample_volume_m = sample_volume_cm/10**6\n",
    "test_xt['pcs_m'] = test_xt['compte']\n",
    "\n",
    "\n",
    "# Filter for Scenario \n",
    "test_xi = test_xt.copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat',  'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions,mse, r2  = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Tous les positions'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-tous-md-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='tous-md-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94bb0f64-eaab-4b39-a648-938aa42a16e8",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "test_xi = test_xt[(test_xt['position'] == 2)].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat', 'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2 = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Plage seche'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-plageseche-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Plage seche'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='plageseche-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e78b009-f725-40d2-a9d5-5906eeb572cc",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "cols = ['échantillon', 'position', 'fréquentation','situation', 'distance', 'substrat']\n",
    "test_xi = test_xt[(test_xt['position'] == 2)&(test_xt.objet == 'fibres')].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat', 'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2 = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Plage seche et fibres'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-plagesechefibres-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Plage seche et fibres'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='plagesechefibres-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a91d8da-3067-4c5a-986e-9daa8bdc4ba4",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "test_xi = test_xt[(test_xt['substrat'] == 1)].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat', 'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2 = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Sables fins'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-sablesfins-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Sables fins'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='sablesfins-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d98ec6b-0f31-4d80-8b94-ac5c73792dfd",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "test_xi = test_xt[(test_xt['substrat'] == 2)].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat', 'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2 = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Cailloux'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-cailloux-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Cailloux'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='cailloux-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "625968dd-abe1-4da3-88d6-7df8e376d3ec",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "test_xi = test_xt[(test_xt['fréquentation'] == 3)].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat', 'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2 = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Fréquentation élevée'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-freq3-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Fréquentation élevée'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='freq3-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "790d9487-2da8-4330-b7b6-6488edb5c367",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Filter for Scenario \n",
    "test_xi = test_xt[(test_xt['fréquentation'] == 2)].copy()\n",
    "test_x = test_xi.groupby(cols, as_index=False).pcs_m.sum()\n",
    "\n",
    "test_x = test_x[['fréquentation', 'situation', 'distance', 'substrat', 'pcs_m']]\n",
    "\n",
    "func = RandomForestRegressor(n_estimators=estimators, criterion=\"absolute_error\", random_state=42)\n",
    "predictions, mse, r2  = analyze_scenario(test_x, func,  n_iterations=iterations, bin_width=0.2)\n",
    "\n",
    "caption = 'Fréquentation moyenne'\n",
    "q_sit_2_freq_3 = makeqdf(test_x.pcs_m.values, predictions, caption=caption)\n",
    "glue('q-freq2-sa', q_sit_2_freq_3, display=False)\n",
    "\n",
    "# the histogram for this scenario:\n",
    "title = 'Plastock 2022, Le Léman\\nDistribution des Prédictions - Sables fins'\n",
    "plot_histogram(predictions, test_x.pcs_m.values, title=title, reference='freq2-sa', display=False, order=\"observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089cbb88-656a-4948-88da-ece0ac79282d",
   "metadata": {},
   "source": [
    "# Micros particules \n",
    "\n",
    "__Format__ \n",
    "\n",
    "Le format suit celui de l'annexe pour les [microplastiques](micro_atts) . Nous incluons également l'analyse utilisant des variables combinées selon la méthode décrite dans la section [Résultats précédents](previous_results).\n",
    "\n",
    "__Le système de mesure.__\n",
    "\n",
    "Dans cette section, les unités sont des déchets par mètre² de rivage: (pcs/m²)\n",
    "\n",
    "## Resumé des résultats\n",
    "\n",
    "\n",
    "### Autres campganes études\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Différences entre les types de plage\n",
    "\n",
    "\n",
    "## Situation\n",
    "\n",
    "::::{grid}\n",
    "\n",
    ":::{grid-item}\n",
    "\n",
    "resumé de [microplastiques](micro_atts) table A1-4 and Fig 3\n",
    "\n",
    "avec détail du table A1-5\n",
    "\n",
    ":::\n",
    "\n",
    ":::{grid-item}\n",
    "Dates d'echantillonage\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dfdd4b-eaa3-408b-99a7-5ef417a215ee",
   "metadata": {},
   "source": [
    "(random_forest_sa_md)=\n",
    "### Random Forest \n",
    "\n",
    "Source : [scikit-learn random forest](https://scikit-learn.org/0.16/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "\n",
    "criterion : `absolute error`\n",
    "\n",
    "La régression avec forêt aléatoire est une technique d'apprentissage automatique (machine learning) utilisée pour prédire des résultats continus (par opposition aux catégories dans la classification). C'est une méthode d'apprentissage ensembliste, ce qui signifie qu'elle combine les prédictions de plusieurs algorithmes d'apprentissage automatique pour produire des prédictions plus précises.\n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Toutes les conditions\n",
    "{glue}`tous-md-sa`\n",
    ":::\n",
    "\n",
    ":::{tab-item} Fréquentation moyenne\n",
    "{glue}`freq2-sa`\n",
    ":::\n",
    "\n",
    ":::{tab-item} Haute fréquentation\n",
    "{glue}`freq3-sa`\n",
    ":::\n",
    "\n",
    ":::{tab-item} Cailloux\n",
    "{glue}`cailloux-sa`\n",
    ":::\n",
    "\n",
    ":::{tab-item} Sables fins\n",
    "{glue}`sablesfins-sa`\n",
    ":::\n",
    "\n",
    ":::{tab-item} Plage seche et sable fins\n",
    "{glue}`plagesechefibres-sa`\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Plage seche\n",
    "{glue}`plageseche-sa`\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Ligne d'eau\n",
    "{glue}`lignedeau-sa`\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    ":::{tab-item} Résultats\n",
    ":selected:\n",
    "\n",
    "````{grid} 1 2 2 2\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-tous-md-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "\n",
    "Les modèles ont fait l'objet d'un bootstrap, 100 itérations pour chaque scénario. Les résultats estimés sont la collection de toutes les prédictions de chaque itération.\n",
    "\n",
    "Par exemple, le tableau intitulé \"Gravier\" présente les résultats observés et prévus pour les plages ayant un substrat de 3 ou 4.\n",
    "\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-freq2-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-freq3-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-cailloux-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-sablesfins-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-plagesechefibres-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-plageseche-sa`\n",
    "```\n",
    "\n",
    "```{grid-item}\n",
    "{glue}`q-lignedeau-sa`\n",
    "```\n",
    "\n",
    "````\n",
    ":::\n",
    "\n",
    "::::\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd29516-479d-47ef-b46a-6bcc0d8f1347",
   "metadata": {},
   "source": [
    "\n",
    "## Substrat\n",
    "\n",
    "Le substrat définit la surface de l'emplacement d'échantillonnage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e50a7c8a-1316-4170-88c2-d70d1f64c3ef",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git repo: https://github.com/hammerdirt-analyst/plastock.git\n",
      "\n",
      "Git branch: dec20\n",
      "\n",
      "scipy     : 1.10.1\n",
      "numpy     : 1.24.2\n",
      "seaborn   : 0.12.2\n",
      "pandas    : 2.0.0\n",
      "matplotlib: 3.7.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions -b -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba80e4d-6655-47b7-91fd-6276e80722b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}