{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13449d70-f334-4082-a3ff-2d6ca7bd1a6a",
   "metadata": {},
   "source": [
    "(data_processing)=\n",
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0416dd59-f9b4-436d-ad6d-2b374c649c19",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import multinomial\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from myst_nb import glue\n",
    "from slugify import slugify\n",
    "\n",
    "from typing import List \n",
    "\n",
    "import plastockconf as psc\n",
    "import plastock as pstk\n",
    "\n",
    "from plastockconf import name_zones, name_frequentation, name_situation\n",
    "from plastockconf import name_substrate, name_distance, table_css_styles, table_css_styles_top\n",
    "\n",
    "from plastock import add_table_to_page, capitalize_x_tick_labels, capitalize_x_and_y_axis_labels, capitalize_legend_components, attribute_summary\n",
    "\n",
    "import reportclass as rc\n",
    "\n",
    "section = \"A\"\n",
    "page = \"5\"\n",
    "\n",
    "label = f'Table {section}{page}-'\n",
    "\n",
    "def make_exportable(data, file_name, cmap='YlOrBr'):\n",
    "    data.fillna(0, inplace=True)\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    sns.heatmap(data=data, vmin=0, vmax=1, cmap=cmap, annot=True, fmt='.2', annot_kws={'size':10}, ax=ax, cbar=False)\n",
    "    plt.tight_layout()\n",
    "    ax.tick_params(which='both', axis='both', bottom=False, left=False)\n",
    "    plt.savefig(file_name, dpi=300)\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "glue('blank_caption', \" \", display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bdbc60-f1ba-4a3e-98af-87384a1b594e",
   "metadata": {},
   "source": [
    "## Déchets sauvages\n",
    "\n",
    "__Cette méthode s'applique aux données sur les déchets de plage, mais pas aux données sur les microplastiques__\n",
    "\n",
    "### Position\n",
    "\n",
    "Les inventaires des déchets de plage menées précédemment sur le lac considéraient la plage comme une unité à part entière. Dans Plastock, chaque plage a été divisée en deux sections, la ligne d'eau et la plage sèche. Pour comparer les données de Plastock aux résultats précédents et à d'autres études de ce type, les résultats de Plastock doivent être transformés en une section pour une plage. En d'autres termes, nous devons combiner la surface des deux sections pour chaque échantillon. Le nombre d'objets par échantillon est ensuite divisé par la surface commune des deux sections.\n",
    "\n",
    "### Substrat\n",
    "\n",
    "Certains échantillons ont des substrats différents pour chaque section de chaque plage. Les sections doivent avoir le même substrat pour chaque section si nous voulons compter la plage comme une unité entière. Pour les douze échantillons qui répondent à ce critère, le substrat a été modifié de 4 à 2 (du gravier au sable grossier).\n",
    "\n",
    "### Objets\n",
    "\n",
    "La variété des objets trouvés lors de chaque enquête est autant le reflet des déchets sur le terrain que du temps alloué aux participants pour compter, trier et identifier chaque objet collecté. Par conséquent, les seuls codes ou objets pris en considération sont ceux qui ont été identifiés pendant Plastock. Les données historiques sont filtrées pour les mêmes codes.\n",
    "\n",
    "#### Combinaison d'objets similaires\n",
    "\n",
    "Certaines catégories d'objets sont combinées. Cela signifie que nous modifions le code de certains éléments pour refléter les tendances de l'échantillonnage et tenir compte des différentes interprétations du protocole. Le tableau suivant résume la façon dont les codes sont combinés :\n",
    "\n",
    "1. Gfrags: g80, g79, g78, g77, g76, g75\n",
    "2. Gfoams: G81, G82, G76\n",
    "3. Gcaps: G21, G22, G23, G24\n",
    "\n",
    "Gfrags comprend toutes les formes de plastiques fragmentés de plus de 5 mm, Gfoams toutes les tailles de polystyrène expansé et Gcaps tous les bouchons en plastique et les anneaux en plastique qui accompagnent chaque bouchon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80c6173-3821-493a-aa58-3112f5192619",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ! read in data !\n",
    "# survey data\n",
    "new_data = pd.read_csv(\"data/end_pipe/macro_current.csv\")\n",
    "\n",
    "# the position, substrate, time, area, length for each sample in the data\n",
    "# ! there are samples that have different substrates on the same sample_id.!\n",
    "# that is position 1 has a different substrate than position 2. for litter \n",
    "# data and comparing to previous results this is unimportant. The value to\n",
    "# compare is the total survey divided by the length or the area. This needs\n",
    "# to be accounted for and changes made to certain samples.\n",
    "beach_data = pd.read_csv(\"data/end_pipe/pstock_beaches_current.csv\")\n",
    "\n",
    "\n",
    "# the feature columns of the survey data that are not dependent on the\n",
    "# position or substrat variables. they are indexed on the Plage column\n",
    "# one row for each sample location\n",
    "beach_datax = pd.read_csv(\"data/end_pipe/asl_beaches.csv\").set_index('Plage')\n",
    "\n",
    "# The code definitions\n",
    "codes = pd.read_csv('data/end_pipe/codes.csv').set_index('code')\n",
    "\n",
    "# the regional labels for each survey location\n",
    "regions = pd.read_csv(\"data/end_pipe/lac_leman_regions.csv\")\n",
    "regions.set_index('slug', drop=True, inplace=True)\n",
    "\n",
    "# the city name of the survey locations\n",
    "city_map = pd.read_csv('data/end_pipe/city_map.csv')\n",
    "city_map.set_index('slug', inplace=True)\n",
    "\n",
    "# translation of common terms into french, german and english\n",
    "language_maps = rc.language_maps()\n",
    "\n",
    "new_column_names = {\n",
    "    \"Position\":\"position\",\n",
    "    \"Substrat\":\"substrat\",\n",
    "    \"Date\":\"date\",\n",
    "    \"Code\":\"code\",\n",
    "    \"Quantité\":\"quantité\",\n",
    "    \"Aire\":\"area\"\n",
    "}\n",
    "\n",
    "# import data and assign new column names and sample_id\n",
    "# the sample_id is the tuple (location, date). Each row\n",
    "# is a unique combinantion of sample_id and code\n",
    "work_data = new_data[[\"Plage\", *new_column_names.keys()]].copy()\n",
    "work_data.rename(columns=new_column_names, inplace=True)\n",
    "work_data[\"slug\"] = work_data.Plage.apply(lambda x: slugify(x))\n",
    "work_data[\"échantillon\"] = list(zip(work_data.slug, work_data['date']))\n",
    "work_data['date'] = pd.to_datetime(work_data[\"date\"], format=\"mixed\", dayfirst=True)\n",
    "work_data.dropna(inplace=True)\n",
    "\n",
    "# type the columns\n",
    "work_data[[\"position\", \"substrat\"]] = work_data[[\"position\", \"substrat\"]].astype(\"int\")\n",
    "work_data['échantillon'] = work_data['échantillon'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf97495-c34a-42a0-be3a-70a8ab4e33d0",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "voi = \"substrat\"\n",
    "vals = \"pcs/m²\"\n",
    "code = \"G27\"\n",
    "loc_dates_t = [\"('clarens', '16.01.2022')\", \"('amphion', '01.02.2022')\"]\n",
    "\n",
    "gfrags_c = codes.loc[codes.parent_code == 'Gfrags'].index\n",
    "\n",
    "start_amph_s = new_data[(new_data.Plage == \"Amphion\")&(new_data[\"Date\"] == \"01.02.2022\")&(new_data.Code == \"G27\")]\n",
    "start_amph_c = new_data[(new_data.Plage == \"Amphion\")&(new_data[\"Date\"] == \"01.02.2022\")&(new_data.Code.isin(gfrags_c))]\n",
    "start_clarens_s = new_data[(new_data.Plage == \"Clarens\")&(new_data[\"Date\"] == '16.01.2022')&(new_data.Code == \"G27\")]\n",
    "start_clarens_c = new_data[(new_data.Plage == \"Clarens\")&(new_data[\"Date\"] == '16.01.2022')&(new_data.Code.isin(gfrags_c))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96cae44-9c27-48fa-95c5-69361bd4b0f8",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "start_data = pd.concat([start_amph_s, start_amph_c, start_clarens_c, start_clarens_s])\n",
    "start_data['pcs/m²'] = start_data[\"Quantité\"]/start_data[\"Aire\"]\n",
    "# start_data.loc[start_data.Code.isin(gfrags_c), 'Code'] = 'Gfrags'\n",
    "sd = start_data[[\"Plage\", \"Aire\", \"Position\", \"Substrat\", \"Date\", \"Code\", \"Quantité\", 'pcs/m²']].copy()\n",
    "sd.reset_index(inplace=True, drop=True)\n",
    "\n",
    "display_columns = [\"Plage\", \"Date\", \"Position\", \"Substrat\", \"Aire\",  \"Code\", \"Quantité\", \"pcs/m²\"]\n",
    "\n",
    "caption = f\"<b>{label}1 :</b> Les résultats de l'échantillon avant la combinaison des codes, des substrats et de la position\"\n",
    "sd = sd[display_columns].style.set_table_styles(table_css_styles).format(**psc.format_kwargs).set_caption(caption).hide(axis=\"index\")\n",
    "\n",
    "glue(\"start_data\", sd, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f9da2c5-3282-4539-b229-c2b05dca2e3d",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ! locate all the duplicate values by sample id and area !\n",
    "# this gives a data frame that has the position and area for\n",
    "# each sample_id\n",
    "total_area_dup = work_data.drop_duplicates(['échantillon', 'area'])\n",
    "\n",
    "# ! combine the surface areas of the position vectors !\n",
    "# sum of the areas for each position at each sample\n",
    "# use the sample_id as index and sum the areas for each postition at each sample\n",
    "total_area = total_area_dup.groupby(['échantillon', 'Plage'], as_index=False).area.sum()\n",
    "\n",
    "# sum of the areas for each position at each sample\n",
    "# use the sample_id as index and sum the areas for each postition at each sample\n",
    "total_area = total_area[['échantillon', 'area']].set_index('échantillon', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98818780-7c53-4e28-82a9-31cb72314d22",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# apply the total area to the work_data, index on sample_id\n",
    "work_data['area_c'] = work_data['échantillon'].apply(lambda x: total_area.loc[x])\n",
    "work_data['area'] = work_data.area_c\n",
    "work_data.drop('area_c', axis=1, inplace=True)\n",
    "# the code total per sample with the combined area\n",
    "work_data = work_data.groupby(['échantillon', 'Plage', 'substrat', 'date', 'area','slug', 'quantité', 'code'], as_index=False)['quantité'].sum()\n",
    "work_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3f3194a-2397-4d31-b819-13a86eab9776",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "mask = (work_data[\"échantillon\"].isin(loc_dates_t))&(work_data.code.isin([*gfrags_c, code]))\n",
    "\n",
    "display_columns = [\"échantillon\", \"Plage\", \"Substrat\", \"Aire\",  \"Code\", \"Quantité\"]\n",
    "d = \"pcs/m²\"\n",
    "\n",
    "test = work_data[mask].copy()\n",
    "test = test.rename(columns={\"substrat\":\"Substrat\", \"area\":\"Aire\", \"date\":\"Date\", \"code\":\"Code\", \"quantité\":\"Quantité\"})\n",
    "test = test[display_columns].copy()\n",
    "# test.reset_index(inplace=True, drop=True)\n",
    "caption = f\"<b>{label}2 :</b> La variable position est supprimée et les surfaces sont combinées pour chaque position de chaque échantillon. Cependant, le substrat à Clarens n'est pas corrigé.\"\n",
    "sd_2 = test.style.set_table_styles(table_css_styles).format(**psc.format_kwargs).set_caption(caption).hide(axis='index')\n",
    "glue(\"start_data_2\", sd_2, display=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee52963c-eb6d-451f-87b4-9069b305456b",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# the code total per sample with the combined area\n",
    "work_data = work_data.groupby(['échantillon', 'Plage', 'substrat', 'date', 'area','slug', 'code'], as_index=False)['quantité'].sum()\n",
    "\n",
    "# add the regional component\n",
    "work_data['region'] = work_data.slug.apply(lambda x: regions.loc[x, 'alabel'])\n",
    "\n",
    "work_data = work_data.groupby(['échantillon', 'Plage', 'substrat', 'date', 'area','slug', 'quantité', 'code'], as_index=False)['quantité'].sum()\n",
    "# get the pcs/m²  for each object at each sample\n",
    "work_data['pcs/m²'] = work_data['quantité']/work_data.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db8c53a7-4cfc-4552-a00a-d196f71bd337",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ! there are samples that have different substrates on the same sample_id.!\n",
    "# there should be one substrate per sample_id. Identify the locations that have duplicate values\n",
    "some_data = work_data.copy()\n",
    "groupby = ['échantillon', voi]\n",
    "data = some_data.groupby(groupby, as_index=False)[vals].sum()\n",
    "\n",
    "# the samples with more than one substrate\n",
    "dd = data[data['échantillon'].duplicated()].copy()\n",
    "# select all the duplicated sample_ids from the work_data\n",
    "duplicated = work_data[work_data['échantillon'].isin(dd['échantillon'].unique())].copy()\n",
    "# select all the duplicated sample_ids from the work_data\n",
    "# change the substrat to [2]\n",
    "duplicated['substrat'] = 2 \n",
    "\n",
    "# select all the values that are not duplicated\n",
    "not_duplicated = work_data[~(work_data['échantillon'].isin(dd['échantillon'].unique()))].copy()\n",
    "\n",
    "# put it back together again\n",
    "work_data = pd.concat([duplicated, not_duplicated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39363708-fbc0-4484-8c2b-e99f4ce4cce3",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "mask = (work_data[\"échantillon\"].isin(loc_dates_t))&(work_data.code.isin([*gfrags_c, code]))\n",
    "\n",
    "display_columns = [\"échantillon\", \"Plage\", \"Substrat\", \"Aire\",  \"Code\", \"Quantité\", \"pcs/m²\"]\n",
    "\n",
    "test = work_data[mask].copy()\n",
    "test = test.rename(columns={\"substrat\":\"Substrat\", \"area\":\"Aire\", \"date\":\"Date\", \"code\":\"Code\", \"quantité\":\"Quantité\"})\n",
    "test = test[display_columns].copy()\n",
    "\n",
    "caption = f\"<b>{label}3 :</b> Après avoir combiné les codes et supprimé la variable de la position, il reste des codes en double pour l'échantillon de Clarens.\"\n",
    "sd_3 = test.style.set_table_styles(table_css_styles).format(**psc.format_kwargs).set_caption(caption).hide(axis=\"index\")\n",
    "glue(\"start_data_3\", sd_3, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd0694d8-9f5f-4ad8-929a-49804d47fd8d",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ! valid codes and definitions !\n",
    "# plastock did not use the same inventory as iqaasl\n",
    "# here we select only the codes in the plastock inventory\n",
    "pcodes = work_data.code.unique()\n",
    "\n",
    "# identify and remove codes for which there is no defintion\n",
    "# if the code is not defined then it can not be used\n",
    "t = [x for x in pcodes if x not in codes.index]\n",
    "wd_ni = work_data[~work_data.code.isin(t)].copy()\n",
    "\n",
    "# ! aggregating to Gfrags, Gcaps and Gfoams !\n",
    "# these items are not well divided into the composite subgroups\n",
    "# for example people often know what a cap is, but whether it \n",
    "# comes from a drink bottle or other type is not well considered\n",
    "# we combine the subcategories into more comprehensive groups.\n",
    "ti = rc.use_gfrags_gfoams_gcaps(wd_ni, codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b312536-835e-49d2-9c4c-4ffbee2c6280",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ! groupby the sample id and code otherwise there are duplicate codes\n",
    "# after aggregating to Gfrags etc..\n",
    "work_data = ti.groupby(['échantillon', 'Plage', 'substrat', 'date', 'area', 'slug', 'code'], as_index=False).agg({'quantité':'sum'})\n",
    "work_data['pcs/m²'] = work_data['quantité']/work_data['area']\n",
    "\n",
    "# accounting for objects not found at a sample:\n",
    "# the codes that were indentified are the unique codes\n",
    "# in the set of data, they are the 'inventory'\n",
    "codes_ip = work_data.code.unique()\n",
    "# the unique samples by id\n",
    "loc_dates = work_data['échantillon'].unique()\n",
    "\n",
    "# a copy for itterating\n",
    "wd = work_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f20caec-1c55-46df-afa6-1c2b0f1b36f6",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "start_data.loc[start_data.Code.isin(gfrags_c), 'Code'] = 'Gfrags'\n",
    "\n",
    "display_columns = [\"Plage\", \"Date\", \"Position\", \"Substrat\", \"Aire\",  \"Code\", \"Quantité\", \"pcs/m²\"]\n",
    "\n",
    "start_data = start_data.groupby([\"Plage\", \"Date\", \"Aire\", \"Code\"], as_index=False)[\"Quantité\"].sum()\n",
    "\n",
    "start_data[\"pcs/m²\"] = start_data[\"Quantité\"]/start_data.Aire\n",
    "sd_x = start_data.groupby([\"Plage\", \"Date\", \"Code\"], as_index=False)[\"pcs/m²\"].mean()\n",
    "\n",
    "caption =  f\"<b>{label}5 :</b> La densité de l'échantillon si les zones et les substrats ne sont pas combinés pour chaque échantillon.\" \n",
    "sd_x = sd_x.style.set_table_styles(table_css_styles).format(**psc.format_kwargs).set_caption(caption).hide(axis=\"index\")\n",
    "glue(\"start_data_4\", sd_x, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dcec826-504d-4ddd-86dc-2b664bc8ef41",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# a copy for itterating\n",
    "wd = work_data.copy()\n",
    "\n",
    "rows = []\n",
    "for a_loc in loc_dates:\n",
    "    r = wd.loc[wd['échantillon'] == a_loc].copy()\n",
    "    r.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    t = r.loc[0][['échantillon', 'Plage', 'substrat', 'date', 'area', 'slug']].values\n",
    "    asamp = [x for x in t]\n",
    "    used_codes = r.code.unique()\n",
    "    unused = [x for x in codes_ip if x not in used_codes]\n",
    "    for element in unused:\n",
    "        arow = [*asamp, element, 0, 0]\n",
    "        rows.append(arow)\n",
    "        \n",
    "\n",
    "# now all the data has the same number of records per sample\n",
    "# for each sample we can now say what was found and what was\n",
    "# not found with respect to all the results\n",
    "work_x = pd.DataFrame(rows, columns=['échantillon', 'Plage', 'substrat', 'date', 'area', 'slug', 'code', 'quantité', 'pcs/m²'])\n",
    "work_data = pd.concat([work_x, work_data])\n",
    "\n",
    "\n",
    "# add the regional component\n",
    "work_data['region'] = work_data.slug.apply(lambda x: regions.loc[x, 'alabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ca1a66c-911a-496e-ae2d-277bee82d5f6",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "mask = (work_data[\"échantillon\"].isin(loc_dates_t))&(work_data.code.isin([\"Gfrags\", \"G27\"]))\n",
    "display_columns = [\"échantillon\", \"Plage\", \"Substrat\", \"Aire\",  \"Code\", \"Quantité\", \"pcs/m²\"]\n",
    "test = work_data[mask].copy()\n",
    "test = test.rename(columns={\"substrat\":\"Substrat\", \"area\":\"Aire\", \"date\":\"Date\", \"code\":\"Code\", \"quantité\":\"Quantité\"})\n",
    "test = test[display_columns].copy()\n",
    "\n",
    "caption =  f\"<b>{label}4 :</b> The density of sample if the areas are combined. The result for each sample is the sum of the different results for the different positions and substrates for each sample.\"\n",
    "sd_5 = test.style.set_table_styles(table_css_styles).format(**psc.format_kwargs).set_caption(caption).hide(axis=\"index\")\n",
    "glue(\"start_data_5\", sd_5, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6e9ced6-3c60-4de3-a8ff-3bdf7375a5f4",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "work_data.to_csv('data/end_pipe/macro_data_msquared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ac9be-e6b2-44ad-a2be-7c6edb04f112",
   "metadata": {},
   "source": [
    "## Exemple\n",
    "\n",
    "### Mètres carrés\n",
    "\n",
    "Pour illustrer les changements apportés aux données, considérons deux échantillons et deux codes (G27, Gfrags). Le premier échantillon est Amphion le 2022-02-01 et le second est Clarens le 2022-01-16. À Amphion, il y a plusieurs valeurs pour les mêmes codes le même jour. Il en va de même pour Clarens, mais à Clarens, il y a également deux substrats pour la plage. Notez que les codes G78 et G79 font partie du code combiné Gfrags.\n",
    "\n",
    "```{glue} start_data\n",
    "```\n",
    "\n",
    "Pour combiner les données, nous devons additionner les surfaces pour chaque échantillon, appliquer la nouvelle surface à tous les comptages d'objets pour cet échantillon et ce jour, et supprimer la variable de position. Pour Amphion, la surface totale est de 342 + 98 = 440 m², pour Clarens 273 + 67 = 340 m².\n",
    "\n",
    "```{glue} start_data_2\n",
    "``` \n",
    "\n",
    "Après avoir combiné tous les totaux de codes pour chaque échantillon et remplacé le substrat 4 par le substrat 2 dans toutes les localités qui ont des valeurs de substrat distinctes pour la position, nous nous retrouvons encore avec des codes en double sur un échantillon.\n",
    "\n",
    "```{glue} start_data_3\n",
    "``` \n",
    "\n",
    "Dans la dernière étape, les codes qui doivent être combinés (G78 et G79) sont placés sous un seul code et les données sont agrégées à l'identifiant de l'échantillon (date et lieu de l'échantillon).  \n",
    "\n",
    "```{glue} start_data_5\n",
    "```\n",
    "\n",
    "#### La différence\n",
    "\n",
    "Si les surfaces des positions ne sont pas combinées, les valeurs attendues sont beaucoup plus faibles. En effet, nous sommes obligés de prendre la moyenne de la densité des deux positions et des substrats pour chaque échantillon. \n",
    "\n",
    "```{glue} start_data_4\n",
    "``` \n",
    "\n",
    "__Problème de communication :__ Cela peut être difficile à communiquer. La moyenne pour une région est la moyenne de la densité totale par échantillon. C'est-à-dire le nombre total d'objets divisé par la surface totale. Cependant, dans ce cas, la moyenne de la région est la moyenne des moyennes par échantillon.\n",
    "\n",
    "Cela signifie que la médiane d'une région serait la médiane de la moyenne des moyennes par échantillon. \n",
    "\n",
    "\n",
    "### Mètres linéaires\n",
    "\n",
    "L'opération de transformation des données en mètres linéaires est différente. Pour convertir toutes les plages en une seule section, nous abandonnons la variable superficie et la remplaçons par les données de longueur fournies. La longueur est la même pour les deux sections de chaque plage (les surfaces sont différentes), nous avons donc divisé la somme de tous les objets par la longueur donnée pour l'échantillon. Ce processus est conforme à la norme décrite dans le guide de surveillance des déchets marins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee325418-ef53-42e0-b3a7-41a0e63f362f",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# reading in data\n",
    "new_datai = pd.read_csv(\"data/end_pipe/macro_current.csv\")\n",
    "beach_data = pd.read_csv(\"data/end_pipe/pstock_beaches_current.csv\")\n",
    "\n",
    "\n",
    "# adding the lenght of each sample to the results\n",
    "length_key = beach_data[[\"Plage\",\"length\"]].drop_duplicates(\"Plage\").set_index(\"Plage\")\n",
    "work_datai = new_datai[[\"Plage\", *new_column_names.keys()]].copy()\n",
    "work_datai.rename(columns=new_column_names, inplace=True)\n",
    "work_datai[\"length\"] = work_datai.Plage.apply(lambda x: length_key.loc[x, \"length\"])\n",
    "\n",
    "# making a sample id from the location and date\n",
    "work_datai[\"slug\"] = work_datai.Plage.apply(lambda x: slugify(x))\n",
    "work_datai[\"echantillon\"] = list(zip(work_datai.slug, work_datai['date']))\n",
    "work_datai['date'] = pd.to_datetime(work_datai[\"date\"], format=\"mixed\", dayfirst=True)\n",
    "work_datai.dropna(inplace=True)\n",
    "\n",
    "# defining the data type of columns\n",
    "work_datai[[\"position\", \"substrat\"]] = work_datai[[\"position\", \"substrat\"]].astype(\"int\")\n",
    "\n",
    "# changing the column name to human readable\n",
    "work_datai['échantillon'] = work_datai['echantillon']\n",
    "work_datai.drop(['echantillon'], inplace=True, axis=1)\n",
    "\n",
    "# removing the position variable from data and\n",
    "# grouping by sample id and code to get the total per sample per code\n",
    "work_datai = work_datai.groupby(['échantillon', 'Plage', 'substrat', 'date', 'length', 'slug', 'code'], as_index=False).agg({'quantité':'sum'})\n",
    "\n",
    "# divide that by length\n",
    "work_datai['pcs/m'] = work_datai['quantité']/work_datai['length']\n",
    "\n",
    "# add the region\n",
    "work_datai['region'] = work_datai.slug.apply(lambda x: regions.loc[x, 'alabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b630682-b5af-4834-aead-886e668ab981",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ! combining the substrates for beaches where there are two substrates !\n",
    "voi = 'substrat'\n",
    "vals = \"pcs/m\"\n",
    "some_data = work_datai.copy()\n",
    "groupby = ['échantillon', voi]\n",
    "data = some_data.groupby(groupby, as_index=False)[vals].sum()\n",
    "\n",
    "# these are the duplicate values that need to be changed\n",
    "dd = data[data['échantillon'].duplicated()].copy()\n",
    "\n",
    "duplicated = work_datai[work_datai['échantillon'].isin(dd['échantillon'].unique())].copy()\n",
    "duplicated['substrat'] = 2 \n",
    "\n",
    "# notduplicated\n",
    "not_duplicated = work_datai[~(work_datai['échantillon'].isin(dd['échantillon'].unique()))].copy()\n",
    "\n",
    "# put it back to gether again\n",
    "work_datai = pd.concat([duplicated, not_duplicated])\n",
    "\n",
    "# groupby the new substrat values and calculate pcs/m\n",
    "work_datai = work_datai.groupby(['échantillon', 'Plage', 'region', 'substrat', 'date', 'length', 'slug', 'code'], as_index=False).agg({'quantité':'sum'})\n",
    "work_datai['pcs/m'] = work_datai['quantité']/work_datai['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d02150b6-e677-4bed-9c12-22a4a1051a30",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ! defining the total inventory of the current data !\n",
    "# accounting for objects not found at a sample:\n",
    "# the codes that were indentified at least once = inventory\n",
    "# defining the components of the inventory\n",
    "codes_ip = work_datai.code.unique()\n",
    "\n",
    "# the unique samples\n",
    "loc_dates = work_datai['échantillon'].unique()\n",
    "\n",
    "# a copy for itterating\n",
    "wd = work_datai.copy()\n",
    "\n",
    "# for each sample (échantillon) indentify the codes that were not\n",
    "# found by indentifying all the codes that were found in all surveys\n",
    "# and removing the codes that were not identified at that sample.\n",
    "# for each unidentified code per sample, add a row with the sample\n",
    "# id and the code. give the row a quantity of zero.\n",
    "rows = []\n",
    "for a_loc in loc_dates:\n",
    "    r = wd.loc[wd['échantillon'] == a_loc].copy()\n",
    "    r.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    t = r.loc[0][['échantillon', 'Plage', 'region', 'substrat', 'date', 'length', 'slug']].values\n",
    "    asamp = [x for x in t]\n",
    "    used_codes = r.code.unique()\n",
    "    unused = [x for x in codes_ip if x not in used_codes]\n",
    "    for element in unused:\n",
    "        arow = [*asamp, element, 0, 0]\n",
    "        rows.append(arow)\n",
    "        \n",
    "\n",
    "work_x = pd.DataFrame(rows, columns=['échantillon', 'Plage', 'region', 'substrat', 'date', 'length', 'slug', 'code', 'quantité', 'pcs/m'])\n",
    "work_datai = pd.concat([work_x, work_datai])\n",
    "\n",
    "work_datai.to_csv(\"data/end_pipe/macro_data_linearm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d068d-c2c7-4194-9985-688745195c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}